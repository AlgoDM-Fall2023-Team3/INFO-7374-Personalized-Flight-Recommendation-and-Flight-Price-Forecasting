{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tvY2zVxCxGqG",
        "outputId": "58de714e-0f77-4d59-de5b-8777cd8c0704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting databricks-sql\n",
            "  Downloading databricks_sql-1.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting databricks-sql-connector==2.2.1 (from databricks-sql)\n",
            "  Downloading databricks_sql_connector-2.2.1-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pystache==0.6.0 (from databricks-sql)\n",
            "  Downloading pystache-0.6.0.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lz4<5.0.0,>=4.0.2 (from databricks-sql-connector==2.2.1->databricks-sql)\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.23.4 (from databricks-sql-connector==2.2.1->databricks-sql)\n",
            "  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: oauthlib<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sql-connector==2.2.1->databricks-sql) (3.2.2)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sql-connector==2.2.1->databricks-sql) (1.5.3)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sql-connector==2.2.1->databricks-sql) (10.0.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.1 in /usr/local/lib/python3.10/dist-packages (from databricks-sql-connector==2.2.1->databricks-sql) (2.31.0)\n",
            "Collecting thrift<0.17.0,>=0.16.0 (from databricks-sql-connector==2.2.1->databricks-sql)\n",
            "  Downloading thrift-0.16.0.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.3.0->databricks-sql-connector==2.2.1->databricks-sql) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.3.0->databricks-sql-connector==2.2.1->databricks-sql) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.18.1->databricks-sql-connector==2.2.1->databricks-sql) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.18.1->databricks-sql-connector==2.2.1->databricks-sql) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.18.1->databricks-sql-connector==2.2.1->databricks-sql) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.18.1->databricks-sql-connector==2.2.1->databricks-sql) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.10/dist-packages (from thrift<0.17.0,>=0.16.0->databricks-sql-connector==2.2.1->databricks-sql) (1.16.0)\n",
            "Building wheels for collected packages: pystache, thrift\n",
            "  Building wheel for pystache (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pystache: filename=pystache-0.6.0-py3-none-any.whl size=83618 sha256=5336b7932e5cd766dc6cb7cad59bac9fe2d721f9a8cd6766f7f7437ab28c33bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/98/2c/84a50139380b5a5d8c0b4dfb92f2d860953c9e8e755507a402\n",
            "  Building wheel for thrift (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thrift: filename=thrift-0.16.0-cp310-cp310-linux_x86_64.whl size=373864 sha256=34f7ba26d915dd0cbf086f3fdec8013a913c107483e5570a15f02eb01e984c4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/f8/d2/acfd995e8247eb0cad372fa6a640a5fcf279ab2ed7c5c4490e\n",
            "Successfully built pystache thrift\n",
            "Installing collected packages: thrift, pystache, numpy, lz4, databricks-sql-connector, databricks-sql\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed databricks-sql-1.0.0 databricks-sql-connector-2.2.1 lz4-4.3.2 numpy-1.23.4 pystache-0.6.0 thrift-0.16.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Collecting importlib-metadata<7,>=1.4 (from streamlit)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.4)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.0.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.13.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, validators, smmap, importlib-metadata, pydeck, gitdb, gitpython, streamlit\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.0\n",
            "    Uninstalling importlib-metadata-7.0.0:\n",
            "      Successfully uninstalled importlib-metadata-7.0.0\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.40 importlib-metadata-6.11.0 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.29.0 validators-0.22.0 watchdog-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install databricks-sql\n",
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pouIuGIgxT7m",
        "outputId": "90961429-fe02-4a8f-fb5d-e83d531e0e65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile mainapp.py\n",
        "\n",
        "import streamlit as st\n",
        "from databricks import sql\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "def get_flight_data():\n",
        "    connection = sql.connect(server_hostname = \"\",\n",
        "                 http_path       = \"\",\n",
        "                 access_token    = \"\")\n",
        "    with connection.cursor() as cursor:\n",
        "      cursor.execute(\"SELECT * FROM flight_recommendation.default.flight\")\n",
        "      result = cursor.fetchall()\n",
        "      data = pd.DataFrame(result)\n",
        "      data.columns = ['id','_c0','to','from','date_to','date_from','pos_overall','url','type','price',\n",
        "      'airline','duration']\n",
        "      columns_titles = ['id','from','date_from','to','date_to','url','type','price',\n",
        "      'airline','duration']\n",
        "      data_f=data.reindex(columns=columns_titles)\n",
        "      return data_f\n",
        "\n",
        "def get_popularity_data():\n",
        "    connection = sql.connect(server_hostname = \"\",\n",
        "                 http_path       = \"\",\n",
        "                 access_token    = \"\")\n",
        "    with connection.cursor() as cursor:\n",
        "      cursor.execute(\"SELECT * FROM flight_recommendation.default.flightpopularity\")\n",
        "      result = cursor.fetchall()\n",
        "      data = pd.DataFrame(result)\n",
        "      data.columns = ['airline', 'popularity_score']\n",
        "      return data\n",
        "\n",
        "def get_user_data():\n",
        "    connection = sql.connect(server_hostname = \"\",\n",
        "                 http_path       = \"\",\n",
        "                 access_token    = \"\")\n",
        "    with connection.cursor() as cursor:\n",
        "      cursor.execute(\"SELECT * FROM flight_recommendation.default.user\")\n",
        "      result = cursor.fetchall()\n",
        "      data = pd.DataFrame(result)\n",
        "      data.columns = ['userid','name','age','email','phone_number','travel_purpose','flight_id','id']\n",
        "      return data\n",
        "\n",
        "\n",
        "def generate_recommendations(price_range, date_range, source, destination, sort_by):\n",
        "\n",
        "    # Filter flights based on user inputs\n",
        "    flight_data = get_flight_data()\n",
        "    popularity_data = get_popularity_data()\n",
        "\n",
        "    date_mask = (flight_data['date_from'] >= date_range[0]) & (flight_data['date_to'] <= date_range[1])\n",
        "    source_dest_mask = ((flight_data['from'].str.contains(source)) & (flight_data['to'].str.contains(destination)))\n",
        "\n",
        "    price_mask = (flight_data['price'].astype(int) >= price_range[0]) & (flight_data['price'].astype(int) <= price_range[1])\n",
        "\n",
        "    filtered_data = flight_data[date_mask & source_dest_mask & price_mask]\n",
        "\n",
        "    # Merge with popularity data and sort by popularity\n",
        "    if not filtered_data.empty:\n",
        "        filtered_data = pd.merge(filtered_data, popularity_data, how='left', on='airline')\n",
        "        if sort_by == 'Popularity':\n",
        "            filtered_data = filtered_data.sort_values(by='popularity_score', ascending=False)\n",
        "        elif sort_by == 'Price: Lowest':\n",
        "            filtered_data = filtered_data.sort_values(by='price', ascending=True)\n",
        "        elif sort_by == 'Price: Highest':\n",
        "            filtered_data = filtered_data.sort_values(by='price', ascending=False)\n",
        "\n",
        "    return pd.DataFrame(filtered_data)\n",
        "\n",
        "def generate_flight_recommendations(user_id, price_range, date_range, source, destination, sort_by, top_k=3):\n",
        "    # Merge flight and user data\n",
        "\n",
        "    flight_data = get_flight_data()\n",
        "    user_data = get_user_data()\n",
        "    user_data['userid'] = user_data['userid'].astype(int)\n",
        "    user_data['flight_id'] = user_data['flight_id'].astype(int)\n",
        "\n",
        "    merged_data = pd.merge(flight_data, user_data, how='left', left_on='id', right_on='flight_id')\n",
        "\n",
        "    merged_data['userid'] = merged_data['userid'].astype(float).fillna(0)\n",
        "    merged_data['flight_id'] = merged_data['flight_id'].astype(float).fillna(0)\n",
        "    merged_data['flight_id'] = merged_data['flight_id'].astype(int)\n",
        "\n",
        "    # Filter flights for the given user\n",
        "    user_flights = merged_data[merged_data['userid'] == user_id]\n",
        "\n",
        "    # Create a user profile based on historical airlines\n",
        "    user_profile = user_flights.groupby('airline').size().reset_index(name='num_trips')\n",
        "\n",
        "    # Content-Based Filtering: Recommend flights based on historical airlines\n",
        "    recommended_airlines = user_flights['airline'].value_counts().index[:top_k].tolist()\n",
        "\n",
        "    # Get entire flight data for the recommended airlines\n",
        "    recommended_flights_data = flight_data[flight_data['airline'].isin(recommended_airlines)]\n",
        "\n",
        "    # Sort recommended flights based on the count of historical flights for each airline\n",
        "    recommended_flights_data['airline_count'] = recommended_flights_data['airline'].map(user_profile.set_index('airline')['num_trips'])\n",
        "    recommended_flights_data = recommended_flights_data.sort_values(by='airline_count', ascending=False).drop('airline_count', axis=1)\n",
        "\n",
        "    if recommended_flights_data.empty:\n",
        "      date_mask = (recommended_flights_data['date_from'] >= date_range[0]) & (recommended_flights_data['date_to'] <= date_range[1])\n",
        "      source_dest_mask = ((recommended_flights_data['from'].str.contains(source)) & (recommended_flights_data['to'].str.contains(destination)))\n",
        "\n",
        "      price_mask = (recommended_flights_data['price'].astype(int) >= price_range[0]) & (recommended_flights_data['price'].astype(int) <= price_range[1])\n",
        "\n",
        "      filtered_data = recommended_flights_data[date_mask & price_mask & source_dest_mask]\n",
        "      return pd.DataFrame(filtered_data)\n",
        "    else:\n",
        "      # get all flights data\n",
        "      date_mask = (flight_data['date_from'] >= date_range[0]) & (flight_data['date_to'] <= date_range[1])\n",
        "      source_dest_mask = ((flight_data['from'].str.contains(source)) & (flight_data['to'].str.contains(destination)))\n",
        "\n",
        "      price_mask = (flight_data['price'].astype(int) >= price_range[0]) & (flight_data['price'].astype(int) <= price_range[1])\n",
        "\n",
        "      filtered_data_1 = flight_data[date_mask & price_mask & source_dest_mask]\n",
        "\n",
        "      # get flights data from present in user history\n",
        "      date_mask = (recommended_flights_data['date_from'] >= date_range[0]) & (recommended_flights_data['date_to'] <= date_range[1])\n",
        "      source_dest_mask = ((recommended_flights_data['from'].str.contains(source)) & (recommended_flights_data['to'].str.contains(destination)))\n",
        "\n",
        "      price_mask = (recommended_flights_data['price'].astype(int) >= price_range[0]) & (recommended_flights_data['price'].astype(int) <= price_range[1])\n",
        "\n",
        "      filtered_data_2 = recommended_flights_data[date_mask & price_mask & source_dest_mask]\n",
        "\n",
        "      filtered_data = pd.concat([filtered_data_1, filtered_data_2])\n",
        "      filtered_data = filtered_data.drop_duplicates(subset=['id'])\n",
        "\n",
        "      if sort_by == 'Price: Lowest':\n",
        "          recommendations = filtered_data.sort_values(by='price', ascending=True)\n",
        "      if sort_by == 'Price: Highest':\n",
        "          recommendations = filtered_data.sort_values(by='price', ascending=False)\n",
        "      return pd.DataFrame(recommendations)\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "st.title('Personalized Flight Recommendation System')\n",
        "tabs = st.tabs([\"New User\", \"Existing User\"])\n",
        "c1 = tabs[0]\n",
        "c2 = tabs[1]\n",
        "\n",
        "with st.sidebar:\n",
        "    min_price = st.number_input('Enter the price range', min_value=0, max_value=10000, value=0,key='A')\n",
        "    max_price = st.number_input('Enter the price range', min_value=0, max_value=10000, value=1000,key='B')\n",
        "\n",
        "    price_range = (min_price, max_price)\n",
        "\n",
        "    start_date = st.date_input('Start date',key='SD')\n",
        "    end_date = st.date_input('End date',key='ED')\n",
        "\n",
        "    date_range = (start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))\n",
        "\n",
        "    source = st.text_input(\"Enter the Source City\",key='src')\n",
        "    destination = st.text_input(\"Enter the Destination City\",key='dest')\n",
        "\n",
        "with c1:\n",
        "  sort_by = st.selectbox(\"Sort by\", [\"Popularity\", \"Price: Lowest\", \"Price: Highest\"],key='S')\n",
        "  if st.button('Recommend Flights',key='B1'):\n",
        "\n",
        "    result = generate_recommendations(price_range, date_range, source, destination, sort_by)\n",
        "\n",
        "    if result.empty:\n",
        "      st.warning(\"No flights found\")\n",
        "    else:\n",
        "      st.write(\"Recommended flights\",result)\n",
        "\n",
        "with c2:\n",
        "  user_id_to_recommend = st.number_input('Enter the user id', min_value=0, max_value=10000, value=0)\n",
        "\n",
        "  sort_by = st.selectbox(\"Sort by\", [\"Price: Lowest\", \"Price: Highest\"],key='S1')\n",
        "  if st.button('Recommend Flights',key=\"B2\"):\n",
        "\n",
        "    result = generate_flight_recommendations(user_id_to_recommend, price_range, date_range, source, destination, sort_by)\n",
        "    if result.empty:\n",
        "      st.warning(\"No flights found\")\n",
        "    else:\n",
        "      st.write(\"Recommended flights\",result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4V8SXr9x32_",
        "outputId": "92454579-e09c-4266-85fc-58fa19db91b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m colab@1.0.0 No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m colab@1.0.0 No repository field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package and audited 22 packages in 0.772s\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n"
          ]
        }
      ],
      "source": [
        "# Create a package.json file with default values\n",
        "!echo '{\"name\": \"colab\", \"version\": \"1.0.0\", \"main\": \"index.js\", \"scripts\": {\"start\": \"echo \\\"Error: no start script specified\\\" && exit 1\"}, \"keywords\": [], \"author\": \"\", \"license\": \"ISC\"}' > /content/package.json\n",
        "\n",
        "# Install localtunnel\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQx5xr6Ux9Tk",
        "outputId": "706607fe-8ccc-47ba-9a56-f961c615df7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.86.149.194\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.348s\n",
            "your url is: https://sweet-ends-relate.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoU0MwMJx_lL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
